{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb5b3b5-e791-4254-b2a7-6607c989fe6c",
   "metadata": {},
   "source": [
    "# MultiStain CycleGAN Training for Histopathology Stain Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d66a5",
   "metadata": {},
   "source": [
    "This notebook implements a complete pipeline to train a MultiStain-CycleGAN model for stain normalization using unaligned H5 datasets. The goal of the model is to learn a mapping that translates histopathology images from one domain (images from a given medical center) to another target domain (the staining style used in the test data). This is important for reducing the variability in stain appearance that can adversely affect downstream tasks such as tumor classification.\n",
    "\n",
    "What does this notebook do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182187ec-723d-48b5-add9-f3cdce655fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T14:35:33.721680Z",
     "iopub.status.busy": "2025-04-03T14:35:33.721374Z",
     "iopub.status.idle": "2025-04-03T14:35:34.659721Z",
     "shell.execute_reply": "2025-04-03T14:35:34.658173Z",
     "shell.execute_reply.started": "2025-04-03T14:35:33.721655Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'multistain_cyclegan_normalization'...\n",
      "remote: Enumerating objects: 136, done.\u001b[K\n",
      "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
      "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
      "remote: Total 136 (delta 27), reused 123 (delta 19), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (136/136), 3.42 MiB | 26.34 MiB/s, done.\n",
      "Resolving deltas: 100% (27/27), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/DBO-DKFZ/multistain_cyclegan_normalization.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f5f2a1-f614-4ba6-8df2-f84d1142c816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T21:38:57.093913Z",
     "iopub.status.busy": "2025-04-06T21:38:57.093682Z",
     "iopub.status.idle": "2025-04-06T21:39:03.782984Z",
     "shell.execute_reply": "2025-04-06T21:39:03.782170Z",
     "shell.execute_reply.started": "2025-04-06T21:38:57.093892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dominate\n",
      "  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Downloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: dominate\n",
      "Successfully installed dominate-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dominate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd34adf-dee2-47e2-9c10-987a02fa0592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T21:40:17.035130Z",
     "iopub.status.busy": "2025-04-06T21:40:17.034802Z",
     "iopub.status.idle": "2025-04-06T21:40:17.047268Z",
     "shell.execute_reply": "2025-04-06T21:40:17.046610Z",
     "shell.execute_reply.started": "2025-04-06T21:40:17.035104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd multistain_cyclegan_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23f110-6dc6-426f-b5cf-d989b41d8f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:05:47.862254Z",
     "iopub.status.busy": "2025-04-03T16:05:47.861880Z",
     "iopub.status.idle": "2025-04-03T16:05:47.869803Z",
     "shell.execute_reply": "2025-04-03T16:05:47.868560Z",
     "shell.execute_reply.started": "2025-04-03T16:05:47.862225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3d9f503-1f91-4c20-9f70-867cb040cb7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:05:56.323705Z",
     "iopub.status.busy": "2025-04-03T16:05:56.323333Z",
     "iopub.status.idle": "2025-04-03T16:05:56.466127Z",
     "shell.execute_reply": "2025-04-03T16:05:56.464499Z",
     "shell.execute_reply.started": "2025-04-03T16:05:56.323676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !mkdir checkps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d648b",
   "metadata": {},
   "source": [
    "We define a custom dataset class (H5Dataset) that loads images from H5 files representing two domains: the source (which combines training and validation sets) and the target (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8361db-21e0-4d9e-a579-6f11e6f492c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T21:40:21.207366Z",
     "iopub.status.busy": "2025-04-06T21:40:21.207084Z",
     "iopub.status.idle": "2025-04-06T21:40:32.913373Z",
     "shell.execute_reply": "2025-04-06T21:40:32.912729Z",
     "shell.execute_reply.started": "2025-04-06T21:40:21.207344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This dataset class loads images from H5 files.\n",
    "# It supports unaligned datasets: the source (domain A) and target (domain B) images are loaded separately.\n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, source_paths, target_paths, transform=None, max_source=None, max_target=None, seed=42, domain_id=None):\n",
    "        super().__init__()\n",
    "        # Make sure source_paths and target_paths are lists\n",
    "        self.source_paths = source_paths if isinstance(source_paths, list) else [source_paths]\n",
    "        self.target_paths = target_paths if isinstance(target_paths, list) else [target_paths]\n",
    "        self.transform = transform\n",
    "        self.seed = seed\n",
    "        self.domain_id = domain_id  # If you want to load only data from a specific domain, you can set this\n",
    "        random.seed(self.seed)\n",
    "        # Gather keys (file identifiers) from the source H5 files;\n",
    "        # we balance labels here if needed and optionally filter by a given domain.\n",
    "        self.source_keys = self._gather_keys(self.source_paths, max_source, balance=True, domain=self.domain_id)\n",
    "        # Gather keys from the target H5 files (e.g., test set) without balancing labels.\n",
    "        self.target_keys = self._gather_keys(self.target_paths, max_target, balance=False)\n",
    "        self.len_source = len(self.source_keys)\n",
    "        self.len_target = len(self.target_keys)\n",
    "\n",
    "    # Internal function to load keys from each h5 file.\n",
    "    # If \"balance\" is True, we separate keys by label (0 and 1) and then sample equally.\n",
    "    # If a domain is specified, we only use keys whose metadata matches the domain.\n",
    "    def _gather_keys(self, paths, max_items, balance=False, domain=None):\n",
    "        collected = []\n",
    "        for fpath in paths:\n",
    "            with h5py.File(fpath, 'r') as file:\n",
    "                keys = list(file.keys())\n",
    "                if domain is not None:\n",
    "                    # Filter keys by checking if the metadata value equals the domain ID.\n",
    "                    filtered = [k for k in keys if int(np.array(file[k]['metadata'])[0]) == domain]\n",
    "                    selected = filtered\n",
    "                elif balance:\n",
    "                    # Balance the dataset by labels (0 and 1)\n",
    "                    keys_by_label = {0: [], 1: []}\n",
    "                    for k in keys:\n",
    "                        try:\n",
    "                            lb = int(np.array(file[k]['label']))\n",
    "                            keys_by_label[lb].append(k)\n",
    "                        except KeyError:\n",
    "                            continue\n",
    "                    if max_items is not None:\n",
    "                        # Shuffle lists and choose an equal number from each label.\n",
    "                        random.shuffle(keys_by_label[0])\n",
    "                        random.shuffle(keys_by_label[1])\n",
    "                        min_count = min(max_items // 2, len(keys_by_label[0]), len(keys_by_label[1]))\n",
    "                        selected = keys_by_label[0][:min_count] + keys_by_label[1][:min_count]\n",
    "                    else:\n",
    "                        selected = keys_by_label[0] + keys_by_label[1]\n",
    "                else:\n",
    "                    # If no balancing is needed, sample randomly if max_items is provided.\n",
    "                    selected = keys if max_items is None else random.sample(keys, min(max_items, len(keys)))\n",
    "                # For each selected key, store a tuple of (file path, key)\n",
    "                collected.extend([(fpath, k) for k in selected])\n",
    "        random.shuffle(collected)\n",
    "        return collected\n",
    "\n",
    "    # The total length of the dataset is the maximum of source and target sizes.\n",
    "    def __len__(self):\n",
    "        return max(len(self.source_keys), len(self.target_keys))\n",
    "\n",
    "    # This method returns a dictionary with images from both domains and their paths.\n",
    "    # The modulo operations allow cycling if one domain has fewer images than the other.\n",
    "    def __getitem__(self, idx):\n",
    "        idx_src = idx % self.len_source\n",
    "        idx_tgt = idx % self.len_target\n",
    "        src_path, src_key = self.source_keys[idx_src]\n",
    "        tgt_path, tgt_key = self.target_keys[idx_tgt]\n",
    "        with h5py.File(src_path, 'r') as fs, h5py.File(tgt_path, 'r') as ft:\n",
    "            img_src = torch.tensor(fs[src_key]['img'][()])\n",
    "            img_tgt = torch.tensor(ft[tgt_key]['img'][()])\n",
    "        # If the image is in channel-last format, we transpose it to channel-first.\n",
    "        if img_src.ndim == 3 and img_src.shape[-1] == 3:\n",
    "            img_src = img_src.permute(2, 0, 1)\n",
    "        if img_tgt.ndim == 3 and img_tgt.shape[-1] == 3:\n",
    "            img_tgt = img_tgt.permute(2, 0, 1)\n",
    "        # Convert pixel values from [0,1] to [-1, 1]\n",
    "        img_src = img_src.float() * 2.0 - 1.0\n",
    "        img_tgt = img_tgt.float() * 2.0 - 1.0\n",
    "        # Apply any provided transformations (e.g. resizing, cropping)\n",
    "        if self.transform:\n",
    "            img_src = self.transform(img_src)\n",
    "            img_tgt = self.transform(img_tgt)\n",
    "        # Return a dictionary containing the images and a string representation of their paths\n",
    "        return {'A': img_src, 'B': img_tgt,\n",
    "                'A_paths': f\"{src_path}:{src_key}\",\n",
    "                'B_paths': f\"{tgt_path}:{tgt_key}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cedbfe",
   "metadata": {},
   "source": [
    "This class instantiates and sets up a MultiStain-CycleGAN model training using the parameters provided by an options object.\n",
    "\n",
    "A training loop iterates over the dataset batches, updates the model's learning rate, optimizes model parameters, prints loss metrics periodically, and saves checkpoints at defined intervals.\n",
    "\n",
    "The objective is for the model to learn how to normalize the stains, converting images from the source domain to match the visual appearance of the target domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04e63e-73f4-4942-8718-b48b4d5cc8e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:22:06.034013Z",
     "iopub.status.busy": "2025-04-03T16:22:06.033612Z",
     "iopub.status.idle": "2025-04-03T16:22:06.043177Z",
     "shell.execute_reply": "2025-04-03T16:22:06.042153Z",
     "shell.execute_reply.started": "2025-04-03T16:22:06.033983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/base_model.py\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Training function for CycleGAN\n",
    "# =======================================================================\n",
    "def cyclegan_training(options):\n",
    "    # Print the options to verify our configuration.\n",
    "    print(\"Training options:\", options)\n",
    "    \n",
    "    # Create the dataset from source and target H5 files.\n",
    "    # The source_paths are our training and validation data, while target_paths is the test data.\n",
    "    # We're not applying any transformation.\n",
    "    train_data = H5Dataset(\n",
    "        source_paths=[options.train_path, options.val_path],\n",
    "        target_paths=options.test_path,\n",
    "        transform=None\n",
    "    )\n",
    "    \n",
    "    # Create a DataLoader to shuffle and batch the dataset.\n",
    "    data_loader = DataLoader(train_data, batch_size=options.batch_size, shuffle=True)\n",
    "    print(f\"Total images: {len(train_data)}\")\n",
    "    \n",
    "    # Instantiate the CycleGAN model and set it up with our options.\n",
    "    cyclegan_model = MultiStainCycleGANModel(options)\n",
    "    cyclegan_model.setup(options)\n",
    "    \n",
    "    # Create a visualizer object to help display training results.\n",
    "    vis = Visualizer(options)\n",
    "    \n",
    "    total_iters = 0  # This will count the total number of images processed.\n",
    "    \n",
    "    # Loop over epochs; the total number of epochs includes epochs with constant learning rate and decay.\n",
    "    for ep in range(options.epoch_count, options.n_epochs + options.n_epochs_decay + 1):\n",
    "        # Update the learning rate as per the scheduler.\n",
    "        cyclegan_model.update_learning_rate()\n",
    "        \n",
    "        start_time = time.time()  # Record the start time for epoch timing.\n",
    "        ep_iter = 0  # Initialize an epoch-specific iteration counter.\n",
    "        print(f\"\\n-- Epoch {ep} --\")\n",
    "        \n",
    "        # Reset visualizer to clear any previous output.\n",
    "        vis.reset()\n",
    "        cyclegan_model.isTrain = True  # Ensure model is in training mode.\n",
    "        \n",
    "        # Iterate over batches provided by the DataLoader.\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            total_iters += options.batch_size\n",
    "            ep_iter += options.batch_size\n",
    "            \n",
    "            # Set the current batch as input for the model.\n",
    "            cyclegan_model.set_input(batch)\n",
    "            # Run one forward and backward pass, update model parameters.\n",
    "            cyclegan_model.optimize_parameters()\n",
    "            \n",
    "            # Every few iterations (as specified by print_freq), print the loss.\n",
    "            if total_iters % options.print_freq == 0:\n",
    "                # Get current losses as a dictionary.\n",
    "                losses = cyclegan_model.get_current_losses()\n",
    "                # Format the loss dictionary into a string for printing.\n",
    "                loss_message = \", \".join([f\"{k}: {v:.4f}\" for k, v in losses.items()])\n",
    "                print(f\"[Epoch {ep} | Iter {ep_iter}] {loss_message}\")\n",
    "                # Get current visuals (images) from the model and display them.\n",
    "                current_visuals = cyclegan_model.get_current_visuals()\n",
    "                vis.display_current_results(current_visuals, ep, save_result=True)\n",
    "        \n",
    "        # Every save_epoch_freq, save the model checkpoints.\n",
    "        if ep % options.save_epoch_freq == 0:\n",
    "            print(f\"Saving checkpoint for epoch {ep}\")\n",
    "            cyclegan_model.save_networks(ep)\n",
    "        \n",
    "        print(f\"Epoch {ep} finished in {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    # Return the trained CycleGAN model.\n",
    "    return cyclegan_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88ccdf",
   "metadata": {},
   "source": [
    "Start the training using the options below. The options are quite the same as the paper's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47fc73-1ef0-4edd-8058-49abcf0af65c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:33:42.457930Z",
     "iopub.status.busy": "2025-04-03T18:33:42.457596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Options: Namespace(dataroot='../', name='experiment_name', gpu_ids=0, model='multistain_cyclegan', direction='AtoB', batch_size=32, input_nc=3, output_nc=3, ngf=64, ndf=64, netG='resnet_9blocks', netD='basic', norm='instance', no_dropout=False, init_type='normal', init_gain=0.02, dataset_mode='unaligned', color_augment=False, brightness=0.0, contrast=0.0, saturation=0.0, hue=0.0, gan_mode='lsgan', pool_size=50, D_thresh=False, D_thresh_value=0.5, n_layers_D=3, lr_policy='linear', lr_decay_iters=50, display_id=0, display_winsize=256, display_port=8097, display_server='http://localhost', display_env='main', display_ncols=4, no_html=False, checkpoints_dir='../checkps', epoch_count=1, n_epochs=20, n_epochs_decay=2, lr_G=0.0002, lr_D=0.0002, beta1=0.5, netD_opt='adam', print_freq=100, save_epoch_freq=1, lambda_A=10.0, lambda_B=10.0, lambda_identity=0.5, max_items_A=None, max_items_B=None, train_path='../../input/mva-dlmi-2025-histopathology-ood-classification/train.h5', val_path='../../input/mva-dlmi-2025-histopathology-ood-classification/val.h5', test_path='../../input/mva-dlmi-2025-histopathology-ood-classification/test.h5', domain=None, isTrain=True)\n",
      "Source domain images: 134904\n",
      "Source domain images: 134904\n",
      "Source domain images: 134904\n",
      "[INFO] Number of training images: 134904\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "create web directory ../checkps/web...\n",
      "\n",
      "üîÅ Starting epoch 1\n",
      "Source domain images: 134904\n",
      "Source domain images: 134904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/multistain_cyclegan_normalization/models/multistain_cyclegan_model.py:109: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler_G = GradScaler()\n",
      "/kaggle/working/multistain_cyclegan_normalization/models/multistain_cyclegan_model.py:110: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler_D = GradScaler()\n",
      "/kaggle/working/multistain_cyclegan_normalization/models/multistain_cyclegan_model.py:233: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/kaggle/working/multistain_cyclegan_normalization/models/multistain_cyclegan_model.py:238: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/kaggle/working/multistain_cyclegan_normalization/models/multistain_cyclegan_model.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Iter 800] D_A: 0.2908, G_A: 0.4988, cycle_A: 4.3898, idt_A: 1.5857, D_B: 0.5103, G_B: 0.9404, cycle_B: 3.3824, idt_B: 2.4091\n",
      "[Epoch 1 | Iter 1600] D_A: 0.3303, G_A: 0.2183, cycle_A: 3.0030, idt_A: 1.1791, D_B: 0.2435, G_B: 0.3233, cycle_B: 2.4801, idt_B: 1.5730\n",
      "[Epoch 1 | Iter 2400] D_A: 0.2052, G_A: 0.4415, cycle_A: 2.5390, idt_A: 1.1865, D_B: 0.2504, G_B: 0.2231, cycle_B: 2.3528, idt_B: 1.2443\n",
      "[Epoch 1 | Iter 3200] D_A: 0.3074, G_A: 0.2012, cycle_A: 2.8466, idt_A: 1.1350, D_B: 0.2072, G_B: 0.3637, cycle_B: 2.4266, idt_B: 1.2769\n",
      "[Epoch 1 | Iter 4000] D_A: 0.3023, G_A: 0.9212, cycle_A: 2.2348, idt_A: 1.0720, D_B: 0.3061, G_B: 0.8874, cycle_B: 2.2868, idt_B: 1.1111\n",
      "[Epoch 1 | Iter 4800] D_A: 0.2422, G_A: 0.2007, cycle_A: 2.4792, idt_A: 0.9183, D_B: 0.3738, G_B: 1.1714, cycle_B: 1.8843, idt_B: 1.1759\n",
      "[Epoch 1 | Iter 5600] D_A: 0.2040, G_A: 0.2926, cycle_A: 2.4087, idt_A: 0.9410, D_B: 0.1649, G_B: 0.4188, cycle_B: 2.0227, idt_B: 1.2148\n",
      "[Epoch 1 | Iter 6400] D_A: 0.1870, G_A: 0.5384, cycle_A: 2.1634, idt_A: 1.0916, D_B: 0.2296, G_B: 0.6045, cycle_B: 2.2743, idt_B: 1.3158\n",
      "[Epoch 1 | Iter 7200] D_A: 0.2197, G_A: 0.3972, cycle_A: 2.3060, idt_A: 0.8929, D_B: 0.2210, G_B: 0.3533, cycle_B: 2.0249, idt_B: 1.0450\n",
      "[Epoch 1 | Iter 8000] D_A: 0.2120, G_A: 0.6200, cycle_A: 2.4080, idt_A: 0.8561, D_B: 0.1486, G_B: 0.3810, cycle_B: 2.1492, idt_B: 1.0431\n",
      "[Epoch 1 | Iter 8800] D_A: 0.1675, G_A: 0.4347, cycle_A: 2.2709, idt_A: 0.8644, D_B: 0.1488, G_B: 0.7268, cycle_B: 2.0614, idt_B: 0.9743\n",
      "[Epoch 1 | Iter 9600] D_A: 1.1712, G_A: 2.6345, cycle_A: 2.0872, idt_A: 0.7928, D_B: 0.1398, G_B: 0.6328, cycle_B: 1.9671, idt_B: 0.9789\n",
      "[Epoch 1 | Iter 10400] D_A: 0.1874, G_A: 0.5064, cycle_A: 1.8775, idt_A: 0.8241, D_B: 1.3488, G_B: 3.3336, cycle_B: 1.8000, idt_B: 0.8465\n",
      "[Epoch 1 | Iter 11200] D_A: 0.1653, G_A: 0.3685, cycle_A: 1.7628, idt_A: 0.8015, D_B: 0.1682, G_B: 0.3386, cycle_B: 1.7563, idt_B: 0.8750\n",
      "[Epoch 1 | Iter 12000] D_A: 0.1805, G_A: 0.4165, cycle_A: 1.7612, idt_A: 0.7733, D_B: 0.1433, G_B: 0.3704, cycle_B: 1.7001, idt_B: 0.8612\n",
      "[Epoch 1 | Iter 12800] D_A: 0.1723, G_A: 0.4905, cycle_A: 1.8648, idt_A: 0.7374, D_B: 0.1226, G_B: 0.5406, cycle_B: 1.5398, idt_B: 0.9185\n",
      "[Epoch 1 | Iter 13600] D_A: 0.1751, G_A: 0.3543, cycle_A: 1.6303, idt_A: 0.8527, D_B: 0.1486, G_B: 0.4578, cycle_B: 1.8942, idt_B: 0.8140\n",
      "[Epoch 1 | Iter 14400] D_A: 0.1622, G_A: 0.4275, cycle_A: 1.7426, idt_A: 0.7386, D_B: 0.0972, G_B: 0.7233, cycle_B: 1.6266, idt_B: 0.8564\n",
      "[Epoch 1 | Iter 15200] D_A: 0.2419, G_A: 0.6285, cycle_A: 1.6611, idt_A: 0.8139, D_B: 0.1617, G_B: 0.3100, cycle_B: 1.8203, idt_B: 0.8383\n",
      "[Epoch 1 | Iter 16000] D_A: 0.1900, G_A: 0.2818, cycle_A: 1.8859, idt_A: 0.7579, D_B: 0.2996, G_B: 0.1642, cycle_B: 1.7926, idt_B: 0.8285\n",
      "[Epoch 1 | Iter 16800] D_A: 0.1679, G_A: 0.9790, cycle_A: 2.3944, idt_A: 0.8128, D_B: 0.1294, G_B: 0.4022, cycle_B: 1.9937, idt_B: 0.9699\n",
      "[Epoch 1 | Iter 17600] D_A: 0.1567, G_A: 0.4614, cycle_A: 1.5295, idt_A: 0.8896, D_B: 0.1306, G_B: 0.6615, cycle_B: 1.8759, idt_B: 0.6972\n",
      "[Epoch 1 | Iter 18400] D_A: 0.1629, G_A: 0.6012, cycle_A: 1.6430, idt_A: 0.7587, D_B: 0.1262, G_B: 0.4945, cycle_B: 1.6567, idt_B: 0.7526\n",
      "[Epoch 1 | Iter 19200] D_A: 0.1820, G_A: 0.7410, cycle_A: 1.5678, idt_A: 0.6898, D_B: 0.1217, G_B: 0.7147, cycle_B: 1.5026, idt_B: 0.6942\n",
      "[Epoch 1 | Iter 20000] D_A: 0.2972, G_A: 0.1265, cycle_A: 1.7285, idt_A: 0.7246, D_B: 0.1236, G_B: 0.6613, cycle_B: 1.5676, idt_B: 0.7577\n",
      "[Epoch 1 | Iter 20800] D_A: 0.1415, G_A: 0.4150, cycle_A: 1.6435, idt_A: 0.7461, D_B: 0.2224, G_B: 0.1842, cycle_B: 1.7252, idt_B: 0.7579\n",
      "[Epoch 1 | Iter 21600] D_A: 0.0855, G_A: 0.6978, cycle_A: 1.8237, idt_A: 0.7046, D_B: 0.1023, G_B: 0.4458, cycle_B: 1.5222, idt_B: 0.7363\n",
      "[Epoch 1 | Iter 22400] D_A: 0.1766, G_A: 0.2974, cycle_A: 2.0625, idt_A: 0.6691, D_B: 0.1347, G_B: 0.2959, cycle_B: 1.6263, idt_B: 0.9667\n",
      "[Epoch 1 | Iter 23200] D_A: 0.1766, G_A: 0.3223, cycle_A: 1.9846, idt_A: 0.6722, D_B: 0.2341, G_B: 0.1890, cycle_B: 1.6737, idt_B: 0.8887\n",
      "[Epoch 1 | Iter 24000] D_A: 0.2186, G_A: 0.4041, cycle_A: 1.5252, idt_A: 0.6789, D_B: 0.1705, G_B: 0.3524, cycle_B: 1.4602, idt_B: 0.7050\n",
      "[Epoch 1 | Iter 24800] D_A: 0.1885, G_A: 0.5172, cycle_A: 1.7839, idt_A: 0.6701, D_B: 0.1474, G_B: 0.5440, cycle_B: 1.5158, idt_B: 0.7990\n",
      "[Epoch 1 | Iter 25600] D_A: 0.2131, G_A: 0.5079, cycle_A: 1.6066, idt_A: 0.6437, D_B: 0.1610, G_B: 0.2583, cycle_B: 1.4942, idt_B: 0.7629\n",
      "[Epoch 1 | Iter 26400] D_A: 0.1891, G_A: 0.2253, cycle_A: 2.1131, idt_A: 0.6679, D_B: 0.1262, G_B: 0.5894, cycle_B: 1.6370, idt_B: 0.7963\n",
      "[Epoch 1 | Iter 27200] D_A: 0.1904, G_A: 0.2869, cycle_A: 1.6371, idt_A: 0.6539, D_B: 0.2292, G_B: 0.2257, cycle_B: 1.4839, idt_B: 0.7248\n",
      "[Epoch 1 | Iter 28000] D_A: 0.1946, G_A: 0.5737, cycle_A: 1.9677, idt_A: 0.6905, D_B: 0.1547, G_B: 0.3916, cycle_B: 1.5876, idt_B: 0.6949\n",
      "[Epoch 1 | Iter 28800] D_A: 0.1929, G_A: 0.4592, cycle_A: 1.5642, idt_A: 0.6976, D_B: 0.1271, G_B: 0.3717, cycle_B: 1.5553, idt_B: 0.6696\n",
      "[Epoch 1 | Iter 29600] D_A: 0.1826, G_A: 0.3856, cycle_A: 1.6998, idt_A: 0.6951, D_B: 0.1713, G_B: 0.2619, cycle_B: 2.0532, idt_B: 0.6920\n",
      "[Epoch 1 | Iter 30400] D_A: 0.1799, G_A: 0.2923, cycle_A: 1.3702, idt_A: 0.6327, D_B: 0.1925, G_B: 0.6347, cycle_B: 1.4222, idt_B: 0.6654\n",
      "[Epoch 1 | Iter 31200] D_A: 0.2502, G_A: 0.8417, cycle_A: 2.4869, idt_A: 0.7321, D_B: 0.1305, G_B: 0.4218, cycle_B: 1.9452, idt_B: 0.8903\n",
      "[Epoch 1 | Iter 32000] D_A: 0.1980, G_A: 0.3301, cycle_A: 1.3574, idt_A: 0.6701, D_B: 0.2114, G_B: 0.1361, cycle_B: 1.5077, idt_B: 0.6193\n",
      "[Epoch 1 | Iter 32800] D_A: 0.1770, G_A: 0.3295, cycle_A: 1.4876, idt_A: 0.7114, D_B: 0.1703, G_B: 0.4260, cycle_B: 1.5410, idt_B: 0.6811\n",
      "[Epoch 1 | Iter 33600] D_A: 0.2123, G_A: 0.2648, cycle_A: 1.6170, idt_A: 0.6214, D_B: 0.1298, G_B: 0.6970, cycle_B: 1.4998, idt_B: 0.6865\n",
      "[Epoch 1 | Iter 34400] D_A: 0.2576, G_A: 0.1394, cycle_A: 1.5805, idt_A: 0.6712, D_B: 0.1273, G_B: 0.4873, cycle_B: 1.4988, idt_B: 0.7072\n",
      "[Epoch 1 | Iter 35200] D_A: 0.2063, G_A: 0.3062, cycle_A: 1.6000, idt_A: 0.5966, D_B: 0.1649, G_B: 0.4636, cycle_B: 1.4001, idt_B: 0.6702\n",
      "[Epoch 1 | Iter 36000] D_A: 0.2745, G_A: 0.0839, cycle_A: 1.4342, idt_A: 0.5675, D_B: 0.1369, G_B: 0.4511, cycle_B: 1.2813, idt_B: 0.6348\n",
      "[Epoch 1 | Iter 36800] D_A: 0.1977, G_A: 0.2890, cycle_A: 1.4493, idt_A: 0.6036, D_B: 0.2094, G_B: 0.6567, cycle_B: 1.3634, idt_B: 0.6913\n",
      "[Epoch 1 | Iter 37600] D_A: 0.3613, G_A: 0.0749, cycle_A: 1.4648, idt_A: 0.5559, D_B: 0.2152, G_B: 0.1895, cycle_B: 1.3534, idt_B: 0.6611\n",
      "[Epoch 1 | Iter 38400] D_A: 0.1694, G_A: 0.2983, cycle_A: 1.9176, idt_A: 0.7777, D_B: 0.1686, G_B: 0.3211, cycle_B: 2.0679, idt_B: 0.8370\n",
      "[Epoch 1 | Iter 39200] D_A: 0.1825, G_A: 0.3547, cycle_A: 1.4074, idt_A: 0.5954, D_B: 0.2103, G_B: 0.1663, cycle_B: 1.3622, idt_B: 0.6057\n",
      "[Epoch 1 | Iter 40000] D_A: 0.3440, G_A: 0.0650, cycle_A: 1.4865, idt_A: 0.6391, D_B: 0.2529, G_B: 0.1408, cycle_B: 1.5071, idt_B: 0.6244\n",
      "[Epoch 1 | Iter 40800] D_A: 0.1502, G_A: 0.4014, cycle_A: 1.5525, idt_A: 0.6261, D_B: 0.1535, G_B: 0.4147, cycle_B: 1.4323, idt_B: 0.7663\n",
      "[Epoch 1 | Iter 41600] D_A: 0.1419, G_A: 0.3456, cycle_A: 2.1458, idt_A: 0.6686, D_B: 0.1913, G_B: 0.3161, cycle_B: 1.6671, idt_B: 0.7350\n",
      "[Epoch 1 | Iter 42400] D_A: 0.1738, G_A: 0.3685, cycle_A: 1.9637, idt_A: 0.6197, D_B: 0.1853, G_B: 0.4692, cycle_B: 1.6011, idt_B: 0.7500\n",
      "[Epoch 1 | Iter 43200] D_A: 0.1777, G_A: 0.2890, cycle_A: 1.4474, idt_A: 0.6261, D_B: 0.1444, G_B: 0.5026, cycle_B: 1.4183, idt_B: 0.6382\n",
      "[Epoch 1 | Iter 44000] D_A: 0.2549, G_A: 0.1504, cycle_A: 1.6200, idt_A: 0.5893, D_B: 0.1804, G_B: 0.3309, cycle_B: 1.3289, idt_B: 0.6723\n",
      "[Epoch 1 | Iter 44800] D_A: 0.3335, G_A: 1.0575, cycle_A: 1.5235, idt_A: 0.5539, D_B: 0.1617, G_B: 0.2138, cycle_B: 1.2752, idt_B: 0.6468\n",
      "[Epoch 1 | Iter 45600] D_A: 0.1503, G_A: 0.3808, cycle_A: 2.2495, idt_A: 0.6166, D_B: 0.1541, G_B: 0.4743, cycle_B: 1.7324, idt_B: 0.8375\n",
      "[Epoch 1 | Iter 46400] D_A: 0.1842, G_A: 0.3183, cycle_A: 1.3250, idt_A: 0.5623, D_B: 0.1706, G_B: 0.4589, cycle_B: 1.3231, idt_B: 0.5911\n",
      "[Epoch 1 | Iter 47200] D_A: 0.2027, G_A: 0.2991, cycle_A: 1.4447, idt_A: 0.5420, D_B: 0.1631, G_B: 0.2596, cycle_B: 1.2258, idt_B: 0.6436\n",
      "[Epoch 1 | Iter 48000] D_A: 0.1863, G_A: 0.4754, cycle_A: 1.2344, idt_A: 0.6054, D_B: 0.1804, G_B: 0.3071, cycle_B: 1.4182, idt_B: 0.5270\n",
      "[Epoch 1 | Iter 48800] D_A: 0.2013, G_A: 0.1859, cycle_A: 1.4868, idt_A: 0.6206, D_B: 0.1298, G_B: 0.4323, cycle_B: 1.5513, idt_B: 0.5647\n",
      "[Epoch 1 | Iter 49600] D_A: 0.1872, G_A: 0.2358, cycle_A: 1.3056, idt_A: 0.5651, D_B: 0.2006, G_B: 0.1798, cycle_B: 1.3022, idt_B: 0.5435\n",
      "[Epoch 1 | Iter 50400] D_A: 0.1962, G_A: 0.2197, cycle_A: 1.3141, idt_A: 0.5104, D_B: 0.2492, G_B: 0.7463, cycle_B: 1.1564, idt_B: 0.5923\n",
      "[Epoch 1 | Iter 51200] D_A: 0.2063, G_A: 0.2342, cycle_A: 1.3369, idt_A: 0.5789, D_B: 0.1876, G_B: 0.2247, cycle_B: 1.2925, idt_B: 0.5594\n",
      "[Epoch 1 | Iter 52000] D_A: 0.2227, G_A: 0.1367, cycle_A: 1.4009, idt_A: 0.5416, D_B: 0.1583, G_B: 0.4717, cycle_B: 1.2728, idt_B: 0.6101\n",
      "[Epoch 1 | Iter 52800] D_A: 0.2266, G_A: 0.1372, cycle_A: 1.4612, idt_A: 0.5758, D_B: 0.1567, G_B: 0.3747, cycle_B: 1.3204, idt_B: 0.5932\n",
      "[Epoch 1 | Iter 53600] D_A: 0.1829, G_A: 0.1859, cycle_A: 1.2738, idt_A: 0.5738, D_B: 0.1499, G_B: 0.3822, cycle_B: 1.2791, idt_B: 0.5563\n",
      "[Epoch 1 | Iter 55200] D_A: 0.1584, G_A: 0.3234, cycle_A: 1.2778, idt_A: 0.5294, D_B: 0.1615, G_B: 0.4224, cycle_B: 1.2294, idt_B: 0.5348\n",
      "[Epoch 1 | Iter 56000] D_A: 0.2179, G_A: 0.1383, cycle_A: 1.4027, idt_A: 0.5997, D_B: 0.1578, G_B: 0.2726, cycle_B: 1.4299, idt_B: 0.5653\n",
      "[Epoch 1 | Iter 56800] D_A: 0.1938, G_A: 0.1889, cycle_A: 1.3617, idt_A: 0.5163, D_B: 0.2256, G_B: 0.1453, cycle_B: 1.1624, idt_B: 0.5931\n",
      "[Epoch 1 | Iter 57600] D_A: 0.1767, G_A: 0.2269, cycle_A: 1.4480, idt_A: 0.5782, D_B: 0.2141, G_B: 0.5162, cycle_B: 1.4625, idt_B: 0.5706\n",
      "[Epoch 1 | Iter 58400] D_A: 0.1747, G_A: 0.3802, cycle_A: 1.1494, idt_A: 0.5296, D_B: 0.2425, G_B: 0.1113, cycle_B: 1.2241, idt_B: 0.5067\n",
      "[Epoch 1 | Iter 59200] D_A: 0.1633, G_A: 0.3572, cycle_A: 1.2129, idt_A: 0.5033, D_B: 0.1443, G_B: 0.4895, cycle_B: 1.1518, idt_B: 0.5084\n",
      "[Epoch 1 | Iter 60000] D_A: 0.2502, G_A: 0.6582, cycle_A: 1.2024, idt_A: 0.5185, D_B: 0.1582, G_B: 0.3619, cycle_B: 1.2368, idt_B: 0.4991\n",
      "[Epoch 1 | Iter 60800] D_A: 0.1755, G_A: 0.2486, cycle_A: 1.1649, idt_A: 0.5055, D_B: 0.1918, G_B: 0.1762, cycle_B: 1.1930, idt_B: 0.4917\n",
      "[Epoch 1 | Iter 61600] D_A: 0.1714, G_A: 0.3244, cycle_A: 1.4315, idt_A: 0.5084, D_B: 0.1436, G_B: 0.3492, cycle_B: 1.1867, idt_B: 0.5456\n",
      "[Epoch 1 | Iter 62400] D_A: 0.2274, G_A: 0.1398, cycle_A: 1.2892, idt_A: 0.5352, D_B: 0.1859, G_B: 0.2860, cycle_B: 1.2776, idt_B: 0.4817\n",
      "[Epoch 1 | Iter 63200] D_A: 0.2005, G_A: 0.1899, cycle_A: 1.1786, idt_A: 0.5446, D_B: 0.2197, G_B: 0.2247, cycle_B: 1.2811, idt_B: 0.4825\n",
      "[Epoch 1 | Iter 64000] D_A: 0.1874, G_A: 0.2706, cycle_A: 1.6394, idt_A: 0.5961, D_B: 0.1223, G_B: 0.3689, cycle_B: 1.5061, idt_B: 0.7200\n",
      "[Epoch 1 | Iter 64800] D_A: 0.1881, G_A: 0.1760, cycle_A: 1.2790, idt_A: 0.5151, D_B: 0.1776, G_B: 0.3793, cycle_B: 1.1876, idt_B: 0.5114\n",
      "[Epoch 1 | Iter 65600] D_A: 0.1622, G_A: 0.2217, cycle_A: 1.2154, idt_A: 0.5037, D_B: 0.1513, G_B: 0.2342, cycle_B: 1.2192, idt_B: 0.4949\n",
      "[Epoch 1 | Iter 66400] D_A: 0.1455, G_A: 0.3563, cycle_A: 1.3155, idt_A: 0.5054, D_B: 0.1614, G_B: 0.6113, cycle_B: 1.1666, idt_B: 0.5217\n",
      "[Epoch 1 | Iter 67200] D_A: 0.3846, G_A: 1.1267, cycle_A: 1.5391, idt_A: 0.5166, D_B: 0.2111, G_B: 0.1747, cycle_B: 1.1615, idt_B: 0.6671\n",
      "[Epoch 1 | Iter 68000] D_A: 0.1516, G_A: 0.3249, cycle_A: 1.3439, idt_A: 0.4744, D_B: 0.1667, G_B: 0.3818, cycle_B: 1.0621, idt_B: 0.5377\n",
      "[Epoch 1 | Iter 68800] D_A: 0.1558, G_A: 0.5774, cycle_A: 1.2735, idt_A: 0.5477, D_B: 0.3360, G_B: 1.1937, cycle_B: 1.3267, idt_B: 0.5182\n",
      "[Epoch 1 | Iter 69600] D_A: 0.1570, G_A: 0.4827, cycle_A: 1.0914, idt_A: 0.4568, D_B: 0.1792, G_B: 0.4338, cycle_B: 1.0317, idt_B: 0.4741\n",
      "[Epoch 1 | Iter 70400] D_A: 0.2259, G_A: 0.6293, cycle_A: 1.2256, idt_A: 0.4695, D_B: 0.1912, G_B: 0.5057, cycle_B: 1.1019, idt_B: 0.4813\n",
      "[Epoch 1 | Iter 71200] D_A: 0.2365, G_A: 0.9263, cycle_A: 1.1702, idt_A: 0.5110, D_B: 0.1713, G_B: 0.2595, cycle_B: 1.2063, idt_B: 0.4991\n",
      "[Epoch 1 | Iter 72000] D_A: 0.1414, G_A: 0.3325, cycle_A: 1.1898, idt_A: 0.5505, D_B: 0.1646, G_B: 0.2085, cycle_B: 1.3579, idt_B: 0.4783\n",
      "[Epoch 1 | Iter 72800] D_A: 0.1935, G_A: 0.1659, cycle_A: 1.0671, idt_A: 0.4730, D_B: 0.1998, G_B: 0.1264, cycle_B: 1.1287, idt_B: 0.4477\n",
      "[Epoch 1 | Iter 73600] D_A: 0.2712, G_A: 0.6792, cycle_A: 1.3648, idt_A: 0.5480, D_B: 0.1692, G_B: 0.3842, cycle_B: 1.2399, idt_B: 0.5189\n",
      "[Epoch 1 | Iter 74400] D_A: 0.1750, G_A: 0.1651, cycle_A: 1.0792, idt_A: 0.4769, D_B: 0.1978, G_B: 0.5383, cycle_B: 1.0524, idt_B: 0.4165\n",
      "[Epoch 1 | Iter 75200] D_A: 0.1730, G_A: 0.3387, cycle_A: 1.2560, idt_A: 0.4448, D_B: 0.1743, G_B: 0.4223, cycle_B: 1.0211, idt_B: 0.5120\n",
      "[Epoch 1 | Iter 76000] D_A: 0.1417, G_A: 0.3775, cycle_A: 1.6347, idt_A: 0.5259, D_B: 0.1034, G_B: 0.9086, cycle_B: 1.2109, idt_B: 0.6639\n",
      "[Epoch 1 | Iter 76800] D_A: 0.1727, G_A: 0.5108, cycle_A: 1.3682, idt_A: 0.5407, D_B: 0.1730, G_B: 0.2552, cycle_B: 1.3320, idt_B: 0.6078\n",
      "[Epoch 1 | Iter 77600] D_A: 0.1405, G_A: 0.3031, cycle_A: 1.2559, idt_A: 0.5064, D_B: 0.1874, G_B: 0.7657, cycle_B: 1.2297, idt_B: 0.4785\n",
      "[Epoch 1 | Iter 78400] D_A: 0.1791, G_A: 0.5179, cycle_A: 1.3156, idt_A: 0.5321, D_B: 0.2356, G_B: 0.1606, cycle_B: 1.2578, idt_B: 0.4769\n",
      "[Epoch 1 | Iter 79200] D_A: 0.1861, G_A: 0.2034, cycle_A: 1.2191, idt_A: 0.4612, D_B: 0.1676, G_B: 0.3041, cycle_B: 1.0496, idt_B: 0.5002\n",
      "[Epoch 1 | Iter 80000] D_A: 0.1152, G_A: 0.4946, cycle_A: 1.2208, idt_A: 0.4670, D_B: 0.1760, G_B: 0.5113, cycle_B: 1.0516, idt_B: 0.5036\n",
      "[Epoch 1 | Iter 80800] D_A: 0.2050, G_A: 0.1939, cycle_A: 1.2883, idt_A: 0.4996, D_B: 0.1847, G_B: 0.2872, cycle_B: 1.1953, idt_B: 0.5253\n",
      "[Epoch 1 | Iter 81600] D_A: 0.1582, G_A: 0.2636, cycle_A: 1.1623, idt_A: 0.5070, D_B: 0.2541, G_B: 0.9001, cycle_B: 1.1975, idt_B: 0.4607\n",
      "[Epoch 1 | Iter 82400] D_A: 0.1406, G_A: 0.4082, cycle_A: 1.2923, idt_A: 0.4474, D_B: 0.0987, G_B: 0.5967, cycle_B: 1.0707, idt_B: 0.5123\n",
      "[Epoch 1 | Iter 83200] D_A: 0.1633, G_A: 0.5398, cycle_A: 1.2439, idt_A: 0.4631, D_B: 0.1916, G_B: 0.2731, cycle_B: 1.1879, idt_B: 0.5052\n",
      "[Epoch 1 | Iter 84000] D_A: 0.1571, G_A: 0.6708, cycle_A: 1.2031, idt_A: 0.4827, D_B: 0.3171, G_B: 0.0644, cycle_B: 1.0969, idt_B: 0.4674\n",
      "[Epoch 1 | Iter 84800] D_A: 0.1572, G_A: 0.2548, cycle_A: 1.5728, idt_A: 0.5040, D_B: 0.2319, G_B: 0.5533, cycle_B: 1.2687, idt_B: 0.5363\n",
      "[Epoch 1 | Iter 85600] D_A: 0.1128, G_A: 0.5408, cycle_A: 1.0526, idt_A: 0.5636, D_B: 0.1997, G_B: 0.4109, cycle_B: 1.2251, idt_B: 0.4433\n",
      "[Epoch 1 | Iter 86400] D_A: 0.4161, G_A: 0.4874, cycle_A: 1.1099, idt_A: 0.4648, D_B: 0.2241, G_B: 0.5171, cycle_B: 1.1063, idt_B: 0.4403\n",
      "[Epoch 1 | Iter 87200] D_A: 0.1012, G_A: 0.3815, cycle_A: 1.6836, idt_A: 0.5544, D_B: 0.1218, G_B: 0.9870, cycle_B: 1.5660, idt_B: 0.5763\n",
      "[Epoch 1 | Iter 88000] D_A: 0.0884, G_A: 0.5648, cycle_A: 1.5441, idt_A: 0.5022, D_B: 0.1775, G_B: 0.1576, cycle_B: 1.1940, idt_B: 0.6507\n",
      "[Epoch 1 | Iter 88800] D_A: 0.1193, G_A: 0.3021, cycle_A: 1.1614, idt_A: 0.5400, D_B: 0.2538, G_B: 0.9159, cycle_B: 1.2569, idt_B: 0.4674\n",
      "[Epoch 1 | Iter 89600] D_A: 0.0861, G_A: 0.6437, cycle_A: 1.1576, idt_A: 0.4686, D_B: 0.1640, G_B: 0.2343, cycle_B: 1.0731, idt_B: 0.4774\n",
      "[Epoch 1 | Iter 90400] D_A: 0.0976, G_A: 0.6019, cycle_A: 1.1335, idt_A: 0.5454, D_B: 0.2059, G_B: 0.4852, cycle_B: 1.3149, idt_B: 0.4549\n",
      "[Epoch 1 | Iter 91200] D_A: 0.1027, G_A: 0.4897, cycle_A: 1.1700, idt_A: 0.4598, D_B: 0.2062, G_B: 0.6376, cycle_B: 1.1656, idt_B: 0.4735\n",
      "[Epoch 1 | Iter 92000] D_A: 0.1427, G_A: 0.3237, cycle_A: 1.3166, idt_A: 0.5131, D_B: 0.1778, G_B: 0.2235, cycle_B: 1.4199, idt_B: 0.4634\n",
      "[Epoch 1 | Iter 92800] D_A: 0.1551, G_A: 0.2866, cycle_A: 1.2394, idt_A: 0.4752, D_B: 0.1165, G_B: 0.3954, cycle_B: 1.2004, idt_B: 0.4682\n",
      "[Epoch 1 | Iter 93600] D_A: 0.1688, G_A: 0.5416, cycle_A: 1.2083, idt_A: 0.4485, D_B: 0.1438, G_B: 0.4788, cycle_B: 1.1163, idt_B: 0.4929\n",
      "[Epoch 1 | Iter 94400] D_A: 0.1676, G_A: 0.5085, cycle_A: 1.1138, idt_A: 0.4908, D_B: 0.1135, G_B: 0.4319, cycle_B: 1.0963, idt_B: 0.4643\n",
      "[Epoch 1 | Iter 95200] D_A: 0.1670, G_A: 0.3304, cycle_A: 1.5016, idt_A: 0.4829, D_B: 0.1771, G_B: 0.2431, cycle_B: 1.1481, idt_B: 0.6151\n",
      "[Epoch 1 | Iter 96000] D_A: 0.1585, G_A: 0.2747, cycle_A: 1.1476, idt_A: 0.4591, D_B: 0.1447, G_B: 0.2212, cycle_B: 1.1926, idt_B: 0.4955\n",
      "[Epoch 1 | Iter 96800] D_A: 0.1275, G_A: 0.3719, cycle_A: 1.0639, idt_A: 0.4435, D_B: 0.2286, G_B: 0.9990, cycle_B: 1.0706, idt_B: 0.4756\n",
      "[Epoch 1 | Iter 97600] D_A: 0.1403, G_A: 0.3433, cycle_A: 1.0437, idt_A: 0.4666, D_B: 0.1361, G_B: 0.2652, cycle_B: 1.0826, idt_B: 0.4375\n",
      "[Epoch 1 | Iter 98400] D_A: 0.1526, G_A: 0.3019, cycle_A: 1.3139, idt_A: 0.4525, D_B: 0.1377, G_B: 0.3188, cycle_B: 1.1403, idt_B: 0.4565\n",
      "[Epoch 1 | Iter 99200] D_A: 0.1176, G_A: 0.3243, cycle_A: 1.2081, idt_A: 0.4892, D_B: 0.0588, G_B: 1.0214, cycle_B: 1.3649, idt_B: 0.4544\n",
      "[Epoch 1 | Iter 100000] D_A: 0.1542, G_A: 0.3963, cycle_A: 1.2179, idt_A: 0.4586, D_B: 0.1481, G_B: 0.4594, cycle_B: 1.0883, idt_B: 0.4641\n",
      "[Epoch 1 | Iter 100800] D_A: 0.1481, G_A: 0.3690, cycle_A: 1.0080, idt_A: 0.4673, D_B: 0.1340, G_B: 0.3876, cycle_B: 1.1040, idt_B: 0.4197\n",
      "[Epoch 1 | Iter 101600] D_A: 0.2028, G_A: 0.5566, cycle_A: 1.1514, idt_A: 0.4682, D_B: 0.1399, G_B: 0.5531, cycle_B: 1.1763, idt_B: 0.4549\n",
      "[Epoch 1 | Iter 102400] D_A: 0.1582, G_A: 0.5249, cycle_A: 1.1281, idt_A: 0.4371, D_B: 0.1444, G_B: 0.4025, cycle_B: 0.9893, idt_B: 0.4508\n",
      "[Epoch 1 | Iter 103200] D_A: 0.1318, G_A: 0.3335, cycle_A: 1.5164, idt_A: 0.4516, D_B: 0.1150, G_B: 0.3385, cycle_B: 1.1336, idt_B: 0.5037\n",
      "[Epoch 1 | Iter 104000] D_A: 0.1193, G_A: 0.4475, cycle_A: 1.1156, idt_A: 0.5037, D_B: 0.0877, G_B: 0.3747, cycle_B: 1.1695, idt_B: 0.4443\n",
      "[Epoch 1 | Iter 104800] D_A: 0.1291, G_A: 0.3003, cycle_A: 1.0852, idt_A: 0.4658, D_B: 0.1038, G_B: 0.4255, cycle_B: 1.0894, idt_B: 0.4365\n",
      "[Epoch 1 | Iter 105600] D_A: 0.1343, G_A: 0.4541, cycle_A: 1.1904, idt_A: 0.4621, D_B: 0.1347, G_B: 0.4197, cycle_B: 1.0794, idt_B: 0.4404\n",
      "[Epoch 1 | Iter 106400] D_A: 0.1427, G_A: 0.5323, cycle_A: 1.8126, idt_A: 0.5273, D_B: 0.1136, G_B: 0.5482, cycle_B: 1.1834, idt_B: 0.6568\n",
      "[Epoch 1 | Iter 107200] D_A: 0.1326, G_A: 0.4005, cycle_A: 1.1188, idt_A: 0.4820, D_B: 0.0791, G_B: 0.4367, cycle_B: 1.1272, idt_B: 0.4697\n",
      "[Epoch 1 | Iter 108000] D_A: 0.1543, G_A: 0.4989, cycle_A: 1.1943, idt_A: 0.4774, D_B: 0.1098, G_B: 0.2806, cycle_B: 1.1326, idt_B: 0.4841\n",
      "[Epoch 1 | Iter 108800] D_A: 0.1573, G_A: 0.3885, cycle_A: 1.6047, idt_A: 0.5246, D_B: 0.1028, G_B: 0.5486, cycle_B: 1.3812, idt_B: 0.5058\n",
      "[Epoch 1 | Iter 109600] D_A: 0.1135, G_A: 0.3769, cycle_A: 1.0209, idt_A: 0.4282, D_B: 0.1155, G_B: 0.3099, cycle_B: 1.0618, idt_B: 0.4145\n",
      "[Epoch 1 | Iter 110400] D_A: 0.1424, G_A: 0.4661, cycle_A: 1.2072, idt_A: 0.4657, D_B: 0.1266, G_B: 0.4766, cycle_B: 1.1984, idt_B: 0.4219\n",
      "[Epoch 1 | Iter 111200] D_A: 0.1504, G_A: 0.2839, cycle_A: 1.1202, idt_A: 0.4319, D_B: 0.1058, G_B: 0.5142, cycle_B: 1.0098, idt_B: 0.4474\n",
      "[Epoch 1 | Iter 112000] D_A: 0.1363, G_A: 0.3121, cycle_A: 1.0300, idt_A: 0.4600, D_B: 0.1006, G_B: 0.3706, cycle_B: 1.1031, idt_B: 0.4332\n",
      "[Epoch 1 | Iter 112800] D_A: 0.1457, G_A: 0.5607, cycle_A: 1.0826, idt_A: 0.4609, D_B: 0.1535, G_B: 0.7424, cycle_B: 1.1241, idt_B: 0.4293\n",
      "[Epoch 1 | Iter 113600] D_A: 0.1297, G_A: 0.6088, cycle_A: 1.1055, idt_A: 0.4499, D_B: 0.1295, G_B: 0.4225, cycle_B: 1.1506, idt_B: 0.4308\n",
      "[Epoch 1 | Iter 114400] D_A: 0.2669, G_A: 0.1090, cycle_A: 1.2232, idt_A: 0.5129, D_B: 0.1418, G_B: 0.2039, cycle_B: 1.1400, idt_B: 0.4349\n",
      "[Epoch 1 | Iter 115200] D_A: 0.1353, G_A: 0.3304, cycle_A: 1.1073, idt_A: 0.4612, D_B: 0.1583, G_B: 0.6083, cycle_B: 1.0618, idt_B: 0.4164\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    # This Options class is used to parse all the command-line arguments that we need for training.\n",
    "    # It combines basic options and training-specific options.\n",
    "    class Options():\n",
    "        def __init__(self):\n",
    "            # We initialize an ArgumentParser with nice formatting for default values.\n",
    "            self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "            self.initialized = False\n",
    "\n",
    "        def initialize(self):\n",
    "           \n",
    "            self.parser.add_argument('--dataroot', default='../', help='(unused in our simplified version)')\n",
    "            self.parser.add_argument('--name', type=str, default='gen', help='experiment name (this folder will store checkpoints)')\n",
    "            self.parser.add_argument('--gpu_ids', type=int, default=0, help='GPU ID to use; -1 for CPU')\n",
    "            self.parser.add_argument('--model', type=str, default='multistain_cyclegan', help='model name')\n",
    "            self.parser.add_argument('--direction', type=str, default='AtoB', help='mapping direction: AtoB or BtoA')\n",
    "            self.parser.add_argument('--batch_size', type=int, default=32, help='size of each mini-batch')\n",
    "            self.parser.add_argument('--input_nc', type=int, default=3, help='number of channels in input images')\n",
    "            self.parser.add_argument('--output_nc', type=int, default=3, help='number of channels in output images')\n",
    "            self.parser.add_argument('--ngf', type=int, default=64, help='number of filters in the last conv layer of G')\n",
    "            self.parser.add_argument('--ndf', type=int, default=64, help='number of filters in the first conv layer of D')\n",
    "            self.parser.add_argument('--netG', type=str, default='resnet_9blocks', help='architecture of the generator')\n",
    "            self.parser.add_argument('--netD', type=str, default='basic', help='architecture of the discriminator')\n",
    "            self.parser.add_argument('--norm', type=str, default='instance', help='normalization type: instance or batch')\n",
    "            self.parser.add_argument('--no_dropout', action='store_true', help='disable dropout in the generator (if true)')\n",
    "            self.parser.add_argument('--init_type', type=str, default='normal', help='weight initialization method')\n",
    "            self.parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for initialization')\n",
    "            self.parser.add_argument('--dataset_mode', type=str, default='unaligned', help='type of dataset (must be unaligned)')\n",
    "            self.parser.add_argument('--color_augment', action='store_true', help='enable color jitter augmentation')\n",
    "            self.parser.add_argument('--brightness', type=float, default=0.0, help='brightness value for augmentation')\n",
    "            self.parser.add_argument('--contrast', type=float, default=0.0, help='contrast value for augmentation')\n",
    "            self.parser.add_argument('--saturation', type=float, default=0.0, help='saturation value for augmentation')\n",
    "            self.parser.add_argument('--hue', type=float, default=0.0, help='hue value for augmentation')\n",
    "            self.parser.add_argument('--gan_mode', type=str, default='lsgan', help='type of GAN loss (lsgan, vanilla, or wgangp)')\n",
    "            self.parser.add_argument('--pool_size', type=int, default=50, help='size of image buffer for generated images')\n",
    "            self.parser.add_argument('--D_thresh', action='store_true', help='use threshold for updating the discriminator')\n",
    "            self.parser.add_argument('--D_thresh_value', type=float, default=0.5, help='threshold value for discriminator update')\n",
    "            self.parser.add_argument('--n_layers_D', type=int, default=3, help='number of layers in the PatchGAN discriminator')\n",
    "            self.parser.add_argument('--lr_policy', type=str, default='linear', help='learning rate scheduler type')\n",
    "            self.parser.add_argument('--lr_decay_iters', type=int, default=50, help='number of iterations between lr decays')\n",
    "            self.parser.add_argument('--display_id', type=int, default=-0, help='window ID for visdom display')\n",
    "            self.parser.add_argument('--display_winsize', type=int, default=256, help='window size for visdom display')\n",
    "            self.parser.add_argument('--display_port', type=int, default=8097, help='port number for visdom server')\n",
    "            self.parser.add_argument('--display_server', type=str, default=\"http://localhost\", help='visdom server address')\n",
    "            self.parser.add_argument('--display_env', type=str, default='main', help='environment name for visdom display')\n",
    "            self.parser.add_argument('--display_ncols', type=int, default=4, help='number of images per row in visdom')\n",
    "            self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results as HTML')\n",
    "            self.parser.add_argument('--checkpoints_dir', type=str, default='../checkps', help='directory where models are saved')\n",
    "\n",
    "            # Training-specific options\n",
    "            self.parser.add_argument('--epoch_count', type=int, default=1, help='starting epoch number')\n",
    "            self.parser.add_argument('--n_epochs', type=int, default=20, help='number of epochs with constant learning rate')\n",
    "            self.parser.add_argument('--n_epochs_decay', type=int, default=2, help='number of epochs with decaying learning rate')\n",
    "            self.parser.add_argument('--lr_G', type=float, default=0.0002, help='learning rate for the generator')\n",
    "            self.parser.add_argument('--lr_D', type=float, default=0.0002, help='learning rate for the discriminator')\n",
    "            self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum parameter for Adam optimizer')\n",
    "            self.parser.add_argument('--netD_opt', type=str, default='adam', help='optimizer for discriminator (adam or sgd)')\n",
    "            self.parser.add_argument('--print_freq', type=int, default=100, help='frequency (in iterations) to print losses')\n",
    "            self.parser.add_argument('--save_epoch_freq', type=int, default=1, help='frequency (in epochs) for saving checkpoints')\n",
    "            self.parser.add_argument('--lambda_A', type=float, default=10.0, help='weight for cycle consistency loss A')\n",
    "            self.parser.add_argument('--lambda_B', type=float, default=10.0, help='weight for cycle consistency loss B')\n",
    "            self.parser.add_argument('--lambda_identity', type=float, default=0.5, help='weight for identity loss')\n",
    "            self.parser.add_argument('--max_items_A', type=int, default=None, help='maximum number of images to load from domain A (train)')\n",
    "            self.parser.add_argument('--max_items_B', type=int, default=None, help='maximum number of images to load from domain B (val)')\n",
    "            self.parser.add_argument('--train_path', type=str, default='../data/train.h5', help='path to the train h5 file')\n",
    "            self.parser.add_argument('--val_path', type=str, default='../data/val.h5', help='path to the validation h5 file')\n",
    "            self.parser.add_argument('--test_path', type=str, default='../data/test.h5', help='path to the test h5 file')\n",
    "            self.parser.add_argument('--domain', type=int, default=None, help='ID of the selected source domain')\n",
    "            self.initialized = True\n",
    "\n",
    "        def parse(self):\n",
    "            if not self.initialized:\n",
    "                self.initialize()\n",
    "            # We use parse_known_args() to ignore extraneous arguments injected by Jupyter\n",
    "            opt, _ = self.parser.parse_known_args()\n",
    "            opt.isTrain = True  # Force training mode\n",
    "            return opt\n",
    "\n",
    "    # Instantiate options and print them for debugging\n",
    "    opt = Options().parse()\n",
    "    import pprint\n",
    "    pprint.pprint(vars(opt))\n",
    "\n",
    "    # Now run the CycleGAN training function with our parsed options.\n",
    "    trained_cyclegan = cyclegan_training(opt)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11165379,
     "sourceId": 93787,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
