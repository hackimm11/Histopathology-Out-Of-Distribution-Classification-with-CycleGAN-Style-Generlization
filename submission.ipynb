{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bbd698f",
   "metadata": {},
   "source": [
    "# Submission Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56fef2",
   "metadata": {},
   "source": [
    "The whole idea is to run an inference like we did intraining but with the test test, using all the trained: CyckeGAN for changing the style to the one we trained, the MLP classifier, and the Dino feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97abe17c",
   "metadata": {},
   "source": [
    "We start by loading the pretrained CycleGAN generator for stain normalization, and defines a test dataset \n",
    "that uses this generator to process each test image before inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from models.multistain_cyclegan_model import networks  # For generator definition\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the root directory for our project.\n",
    "root_path = \"../\"\n",
    "\n",
    "# Helper function to load the pretrained generator model.\n",
    "def load_pretrained_generator(ckpt_path):\n",
    "    # Define the generator using our chosen architecture.\n",
    "    gen = networks.define_G(3, 3, 64, 'resnet_9blocks', 'instance', True, \"normal\", 0.02, [0])\n",
    "    # Load saved weights from the checkpoint.\n",
    "    gen_state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    gen.load_state_dict(gen_state)\n",
    "    # Set the generator to eval mode for inference.\n",
    "    gen.eval()\n",
    "    return gen\n",
    "\n",
    "# Build the full path to the generator checkpoint and load it.\n",
    "gen_ckpt = os.path.join(root_path, \"checkps/gen/netG_A_epoch6.pth\")\n",
    "gen_normalizer = load_pretrained_generator(gen_ckpt)\n",
    "\n",
    "# Define a dataset class for test images using the pretrained generator for stain normalization.\n",
    "class TestDatasetForSubmission(Dataset):\n",
    "    def __init__(self, h5_path, transform=None, generator=None):\n",
    "        self.h5_path = h5_path              # Path to the H5 file with test images.\n",
    "        self.transform = transform          # Optional transformation (e.g., resizing).\n",
    "        self.generator = generator          # Pretrained generator to normalize the stain.\n",
    "        with h5py.File(self.h5_path, 'r') as f:\n",
    "            self.ids = list(f.keys())       # Retrieve all image IDs.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)               # Total number of test images.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.ids[idx]                # Get image key.\n",
    "        with h5py.File(self.h5_path, 'r') as f:\n",
    "            img = torch.tensor(f[key]['img'][()]).float()  # Load image data.\n",
    "        # Convert image to channel-first if needed.\n",
    "        if img.ndim == 3 and img.shape[-1] == 3:\n",
    "            img = img.permute(2, 0, 1)\n",
    "        # Apply transformation if provided.\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        # Scale pixel values to the range [-1, 1].\n",
    "        img = img * 2.0 - 1.0\n",
    "        # Pass the image through the generator for normalization.\n",
    "        if self.generator is not None:\n",
    "            img = self.generator(img.unsqueeze(0))\n",
    "            img = (img + 1) / 2.0  # Rescale normalized output to [0, 1].\n",
    "            img = img.squeeze(0)\n",
    "        # Return the normalized image and its corresponding key as an integer.\n",
    "        return img, int(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d57c2",
   "metadata": {},
   "source": [
    "We do the same as we did in classification including the model definition, the feature extractor, in order to load the trained model for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da88428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission_transform = transforms.Compose([\n",
    "    transforms.Resize((98, 98))\n",
    "])\n",
    "\n",
    "# Build the test dataset and DataLoader with the pretrained generator for stain normalization.\n",
    "test_dataset = TestDatasetForSubmission(os.path.join(root_path, \"data/test.h5\"), \n",
    "                                          transform=submission_transform, \n",
    "                                          generator=gen_normalizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Define an MLP classifier with three hidden layers.\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(128, out_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "# Load the pretrained DINO model for feature extraction and move it to GPU.\n",
    "dino_net = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(\"cuda\")\n",
    "# Freeze all layers except we trained in classification \n",
    "for name, param in dino_net.named_parameters():\n",
    "    if any(sub in name for sub in [\"blocks.9\", \"blocks.10\", \"blocks.11\", \"norm\", \"head\"]):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605d24c",
   "metadata": {},
   "source": [
    "We load the pretrained classifier and fine-tuned DINO model from checkpoints, then runs inference on the test dataset and generates a CSV file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp_model = MLPClassifier(in_features=384, out_features=1).to(\"cuda\")\n",
    "cls_ckpt = os.path.join(root_path, \"checkps/classif/best_classifier_paper.pth\")\n",
    "mlp_model.load_state_dict(torch.load(cls_ckpt, map_location=\"cuda\"))\n",
    "mlp_model.eval()\n",
    "\n",
    "# Load fine-tuned DINO weights if available and update the DINO model state.\n",
    "dino_ckpt = os.path.join(root_path, \"checkps/classif/best_finetuned_dino_layers_paper.pth\")\n",
    "if os.path.exists(dino_ckpt):\n",
    "    ft_weights = torch.load(dino_ckpt, map_location=\"cuda\")\n",
    "    dino_state = dino_net.state_dict()\n",
    "    dino_state.update(ft_weights)\n",
    "    dino_net.load_state_dict(dino_state)\n",
    "dino_net.eval()\n",
    "\n",
    "# Run inference on the test dataset and collect predictions.\n",
    "all_predictions = {'ID': [], 'Pred': []}\n",
    "with torch.no_grad():\n",
    "    for imgs, ids in tqdm(test_loader, desc=\"Running Inference\"):\n",
    "        imgs = imgs.to(\"cuda\")\n",
    "        imgs = submission_transform(imgs)  # Ensure images are resized as needed\n",
    "        feats = dino_net(imgs)              # Extract features using DINO\n",
    "        outputs = mlp_model(feats)          # Get classifier outputs\n",
    "        preds = (outputs.cpu().numpy() > 0.5).astype(int)  # Threshold at 0.5 for binary classification\n",
    "        \n",
    "        for idx, image_id in enumerate(ids):\n",
    "            all_predictions['ID'].append(int(image_id))\n",
    "            all_predictions['Pred'].append(int(preds[idx]))\n",
    "\n",
    "# Create a DataFrame from predictions and save it as a CSV file for submission.\n",
    "submission_df = pd.DataFrame(all_predictions).set_index(\"ID\")\n",
    "submission_file = \"submission.csv\"\n",
    "submission_df.to_csv(submission_file)\n",
    "print(f\"Submission file created at: {submission_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
